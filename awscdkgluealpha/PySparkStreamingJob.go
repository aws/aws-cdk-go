package awscdkgluealpha

import (
	_init_ "github.com/aws/aws-cdk-go/awscdkgluealpha/v2/jsii"
	_jsii_ "github.com/aws/jsii-runtime-go/runtime"

	"github.com/aws/aws-cdk-go/awscdk/v2"
	"github.com/aws/aws-cdk-go/awscdk/v2/awscloudwatch"
	"github.com/aws/aws-cdk-go/awscdk/v2/awsevents"
	"github.com/aws/aws-cdk-go/awscdk/v2/awsiam"
	"github.com/aws/aws-cdk-go/awscdk/v2/interfaces"
	"github.com/aws/constructs-go/constructs/v10"
)

// Python Spark Streaming Jobs class.
//
// A Streaming job is similar to an ETL job, except that it performs ETL on data streams
// using the Apache Spark Structured Streaming framework.
// These jobs will default to use Python 3.9.
//
// Similar to ETL jobs, streaming job supports Scala and Python languages. Similar to ETL,
// it supports G1 and G2 worker type and 2.0, 3.0 and 4.0 version. We’ll default to G2 worker
// and 4.0 version for streaming jobs which developers can override.
// We will enable —enable-metrics, —enable-spark-ui, —enable-continuous-cloudwatch-log.
//
// Example:
//   import cdk "github.com/aws/aws-cdk-go/awscdk"
//   import iam "github.com/aws/aws-cdk-go/awscdk"
//   var stack Stack
//   var role IRole
//   var script Code
//
//   glue.NewPySparkStreamingJob(stack, jsii.String("ImportedJob"), &PySparkStreamingJobProps{
//   	Role: Role,
//   	Script: Script,
//   })
//
// Experimental.
type PySparkStreamingJob interface {
	SparkJob
	// The environment this resource belongs to.
	//
	// For resources that are created and managed in a Stack (those created by
	// creating new class instances like `new Role()`, `new Bucket()`, etc.), this
	// is always the same as the environment of the stack they belong to.
	//
	// For referenced resources (those obtained from referencing methods like
	// `Role.fromRoleArn()`, `Bucket.fromBucketName()`, etc.), they might be
	// different than the stack they were imported into.
	// Experimental.
	Env() *interfaces.ResourceEnvironment
	// The principal to grant permissions to.
	// Experimental.
	GrantPrincipal() awsiam.IPrincipal
	// The ARN of the job.
	// Experimental.
	JobArn() *string
	// The name of the job.
	// Experimental.
	JobName() *string
	// The tree node.
	// Experimental.
	Node() constructs.Node
	// Returns a string-encoded token that resolves to the physical name that should be passed to the CloudFormation resource.
	//
	// This value will resolve to one of the following:
	// - a concrete value (e.g. `"my-awesome-bucket"`)
	// - `undefined`, when a name should be generated by CloudFormation
	// - a concrete name generated automatically during synthesis, in
	//   cross-environment scenarios.
	// Experimental.
	PhysicalName() *string
	// The IAM role Glue assumes to run this job.
	// Experimental.
	Role() awsiam.IRole
	// The Spark UI logs location if Spark UI monitoring and debugging is enabled.
	// See: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html
	//
	// Experimental.
	SparkUILoggingLocation() *SparkUILoggingLocation
	// The stack in which this resource is defined.
	// Experimental.
	Stack() awscdk.Stack
	// Apply the given removal policy to this resource.
	//
	// The Removal Policy controls what happens to this resource when it stops
	// being managed by CloudFormation, either because you've removed it from the
	// CDK application or because you've made a change that requires the resource
	// to be replaced.
	//
	// The resource can be deleted (`RemovalPolicy.DESTROY`), or left in your AWS
	// account for data recovery and cleanup later (`RemovalPolicy.RETAIN`).
	// Experimental.
	ApplyRemovalPolicy(policy awscdk.RemovalPolicy)
	// Returns the job arn.
	// Experimental.
	BuildJobArn(scope constructs.Construct, jobName *string) *string
	// Check no usage of reserved arguments.
	// See: https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html
	//
	// Experimental.
	CheckNoReservedArgs(defaultArguments *map[string]*string) *map[string]*string
	// Experimental.
	CodeS3ObjectUrl(code Code) *string
	// Experimental.
	GeneratePhysicalName() *string
	// Returns an environment-sensitive token that should be used for the resource's "ARN" attribute (e.g. `bucket.bucketArn`).
	//
	// Normally, this token will resolve to `arnAttr`, but if the resource is
	// referenced across environments, `arnComponents` will be used to synthesize
	// a concrete ARN with the resource's physical name. Make sure to reference
	// `this.physicalName` in `arnComponents`.
	// Experimental.
	GetResourceArnAttribute(arnAttr *string, arnComponents *awscdk.ArnComponents) *string
	// Returns an environment-sensitive token that should be used for the resource's "name" attribute (e.g. `bucket.bucketName`).
	//
	// Normally, this token will resolve to `nameAttr`, but if the resource is
	// referenced across environments, it will be resolved to `this.physicalName`,
	// which will be a concrete name.
	// Experimental.
	GetResourceNameAttribute(nameAttr *string) *string
	// Create a CloudWatch metric.
	// See: https://docs.aws.amazon.com/glue/latest/dg/monitoring-awsglue-with-cloudwatch-metrics.html
	//
	// Experimental.
	Metric(metricName *string, type_ MetricType, props *awscloudwatch.MetricOptions) awscloudwatch.Metric
	// Return a CloudWatch Metric indicating job failure.
	//
	// This metric is based on the Rule returned by no-args onFailure() call.
	// Experimental.
	MetricFailure(props *awscloudwatch.MetricOptions) awscloudwatch.Metric
	// Return a CloudWatch Metric indicating job success.
	//
	// This metric is based on the Rule returned by no-args onSuccess() call.
	// Experimental.
	MetricSuccess(props *awscloudwatch.MetricOptions) awscloudwatch.Metric
	// Return a CloudWatch Metric indicating job timeout.
	//
	// This metric is based on the Rule returned by no-args onTimeout() call.
	// Experimental.
	MetricTimeout(props *awscloudwatch.MetricOptions) awscloudwatch.Metric
	// Experimental.
	NonExecutableCommonArguments(props *SparkJobProps) *map[string]*string
	// Create a CloudWatch Event Rule for this Glue Job when it's in a given state.
	// See: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#glue-event-types
	//
	// Experimental.
	OnEvent(id *string, options *awsevents.OnEventOptions) awsevents.Rule
	// Return a CloudWatch Event Rule matching FAILED state.
	// Experimental.
	OnFailure(id *string, options *awsevents.OnEventOptions) awsevents.Rule
	// Create a CloudWatch Event Rule for the transition into the input jobState.
	// Experimental.
	OnStateChange(id *string, jobState JobState, options *awsevents.OnEventOptions) awsevents.Rule
	// Create a CloudWatch Event Rule matching JobState.SUCCEEDED.
	// Experimental.
	OnSuccess(id *string, options *awsevents.OnEventOptions) awsevents.Rule
	// Return a CloudWatch Event Rule matching TIMEOUT state.
	// Experimental.
	OnTimeout(id *string, options *awsevents.OnEventOptions) awsevents.Rule
	// Setup Continuous Logging Properties.
	//
	// Returns: String containing the args for the continuous logging command.
	// Experimental.
	SetupContinuousLogging(role awsiam.IRole, props *ContinuousLoggingProps) interface{}
	// Set the arguments for extra {@link Code}-related properties.
	// Experimental.
	SetupExtraCodeArguments(args *map[string]*string, props *SparkExtraCodeProps)
	// Returns a string representation of this construct.
	// Experimental.
	ToString() *string
	// Applies one or more mixins to this construct.
	//
	// Mixins are applied in order. The list of constructs is captured at the
	// start of the call, so constructs added by a mixin will not be visited.
	// Use multiple `with()` calls if subsequent mixins should apply to added
	// constructs.
	//
	// Returns: This construct for chaining.
	// Experimental.
	With(mixins ...constructs.IMixin) constructs.IConstruct
}

// The jsii proxy struct for PySparkStreamingJob
type jsiiProxy_PySparkStreamingJob struct {
	jsiiProxy_SparkJob
}

func (j *jsiiProxy_PySparkStreamingJob) Env() *interfaces.ResourceEnvironment {
	var returns *interfaces.ResourceEnvironment
	_jsii_.Get(
		j,
		"env",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) GrantPrincipal() awsiam.IPrincipal {
	var returns awsiam.IPrincipal
	_jsii_.Get(
		j,
		"grantPrincipal",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) JobArn() *string {
	var returns *string
	_jsii_.Get(
		j,
		"jobArn",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) JobName() *string {
	var returns *string
	_jsii_.Get(
		j,
		"jobName",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) Node() constructs.Node {
	var returns constructs.Node
	_jsii_.Get(
		j,
		"node",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) PhysicalName() *string {
	var returns *string
	_jsii_.Get(
		j,
		"physicalName",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) Role() awsiam.IRole {
	var returns awsiam.IRole
	_jsii_.Get(
		j,
		"role",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) SparkUILoggingLocation() *SparkUILoggingLocation {
	var returns *SparkUILoggingLocation
	_jsii_.Get(
		j,
		"sparkUILoggingLocation",
		&returns,
	)
	return returns
}

func (j *jsiiProxy_PySparkStreamingJob) Stack() awscdk.Stack {
	var returns awscdk.Stack
	_jsii_.Get(
		j,
		"stack",
		&returns,
	)
	return returns
}


// PySparkStreamingJob constructor.
// Experimental.
func NewPySparkStreamingJob(scope constructs.Construct, id *string, props *PySparkStreamingJobProps) PySparkStreamingJob {
	_init_.Initialize()

	if err := validateNewPySparkStreamingJobParameters(scope, id, props); err != nil {
		panic(err)
	}
	j := jsiiProxy_PySparkStreamingJob{}

	_jsii_.Create(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		[]interface{}{scope, id, props},
		&j,
	)

	return &j
}

// PySparkStreamingJob constructor.
// Experimental.
func NewPySparkStreamingJob_Override(p PySparkStreamingJob, scope constructs.Construct, id *string, props *PySparkStreamingJobProps) {
	_init_.Initialize()

	_jsii_.Create(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		[]interface{}{scope, id, props},
		p,
	)
}

// Identifies an existing Glue Job from a subset of attributes that can be referenced from within another Stack or Construct.
// Experimental.
func PySparkStreamingJob_FromJobAttributes(scope constructs.Construct, id *string, attrs *JobAttributes) IJob {
	_init_.Initialize()

	if err := validatePySparkStreamingJob_FromJobAttributesParameters(scope, id, attrs); err != nil {
		panic(err)
	}
	var returns IJob

	_jsii_.StaticInvoke(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		"fromJobAttributes",
		[]interface{}{scope, id, attrs},
		&returns,
	)

	return returns
}

// Checks if `x` is a construct.
//
// Use this method instead of `instanceof` to properly detect `Construct`
// instances, even when the construct library is symlinked.
//
// Explanation: in JavaScript, multiple copies of the `constructs` library on
// disk are seen as independent, completely different libraries. As a
// consequence, the class `Construct` in each copy of the `constructs` library
// is seen as a different class, and an instance of one class will not test as
// `instanceof` the other class. `npm install` will not create installations
// like this, but users may manually symlink construct libraries together or
// use a monorepo tool: in those cases, multiple copies of the `constructs`
// library can be accidentally installed, and `instanceof` will behave
// unpredictably. It is safest to avoid using `instanceof`, and using
// this type-testing method instead.
//
// Returns: true if `x` is an object created from a class which extends `Construct`.
// Experimental.
func PySparkStreamingJob_IsConstruct(x interface{}) *bool {
	_init_.Initialize()

	if err := validatePySparkStreamingJob_IsConstructParameters(x); err != nil {
		panic(err)
	}
	var returns *bool

	_jsii_.StaticInvoke(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		"isConstruct",
		[]interface{}{x},
		&returns,
	)

	return returns
}

// Returns true if the construct was created by CDK, and false otherwise.
// Experimental.
func PySparkStreamingJob_IsOwnedResource(construct constructs.IConstruct) *bool {
	_init_.Initialize()

	if err := validatePySparkStreamingJob_IsOwnedResourceParameters(construct); err != nil {
		panic(err)
	}
	var returns *bool

	_jsii_.StaticInvoke(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		"isOwnedResource",
		[]interface{}{construct},
		&returns,
	)

	return returns
}

// Check whether the given construct is a Resource.
// Experimental.
func PySparkStreamingJob_IsResource(construct constructs.IConstruct) *bool {
	_init_.Initialize()

	if err := validatePySparkStreamingJob_IsResourceParameters(construct); err != nil {
		panic(err)
	}
	var returns *bool

	_jsii_.StaticInvoke(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		"isResource",
		[]interface{}{construct},
		&returns,
	)

	return returns
}

func PySparkStreamingJob_PROPERTY_INJECTION_ID() *string {
	_init_.Initialize()
	var returns *string
	_jsii_.StaticGet(
		"@aws-cdk/aws-glue-alpha.PySparkStreamingJob",
		"PROPERTY_INJECTION_ID",
		&returns,
	)
	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) ApplyRemovalPolicy(policy awscdk.RemovalPolicy) {
	if err := p.validateApplyRemovalPolicyParameters(policy); err != nil {
		panic(err)
	}
	_jsii_.InvokeVoid(
		p,
		"applyRemovalPolicy",
		[]interface{}{policy},
	)
}

func (p *jsiiProxy_PySparkStreamingJob) BuildJobArn(scope constructs.Construct, jobName *string) *string {
	if err := p.validateBuildJobArnParameters(scope, jobName); err != nil {
		panic(err)
	}
	var returns *string

	_jsii_.Invoke(
		p,
		"buildJobArn",
		[]interface{}{scope, jobName},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) CheckNoReservedArgs(defaultArguments *map[string]*string) *map[string]*string {
	var returns *map[string]*string

	_jsii_.Invoke(
		p,
		"checkNoReservedArgs",
		[]interface{}{defaultArguments},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) CodeS3ObjectUrl(code Code) *string {
	if err := p.validateCodeS3ObjectUrlParameters(code); err != nil {
		panic(err)
	}
	var returns *string

	_jsii_.Invoke(
		p,
		"codeS3ObjectUrl",
		[]interface{}{code},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) GeneratePhysicalName() *string {
	var returns *string

	_jsii_.Invoke(
		p,
		"generatePhysicalName",
		nil, // no parameters
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) GetResourceArnAttribute(arnAttr *string, arnComponents *awscdk.ArnComponents) *string {
	if err := p.validateGetResourceArnAttributeParameters(arnAttr, arnComponents); err != nil {
		panic(err)
	}
	var returns *string

	_jsii_.Invoke(
		p,
		"getResourceArnAttribute",
		[]interface{}{arnAttr, arnComponents},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) GetResourceNameAttribute(nameAttr *string) *string {
	if err := p.validateGetResourceNameAttributeParameters(nameAttr); err != nil {
		panic(err)
	}
	var returns *string

	_jsii_.Invoke(
		p,
		"getResourceNameAttribute",
		[]interface{}{nameAttr},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) Metric(metricName *string, type_ MetricType, props *awscloudwatch.MetricOptions) awscloudwatch.Metric {
	if err := p.validateMetricParameters(metricName, type_, props); err != nil {
		panic(err)
	}
	var returns awscloudwatch.Metric

	_jsii_.Invoke(
		p,
		"metric",
		[]interface{}{metricName, type_, props},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) MetricFailure(props *awscloudwatch.MetricOptions) awscloudwatch.Metric {
	if err := p.validateMetricFailureParameters(props); err != nil {
		panic(err)
	}
	var returns awscloudwatch.Metric

	_jsii_.Invoke(
		p,
		"metricFailure",
		[]interface{}{props},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) MetricSuccess(props *awscloudwatch.MetricOptions) awscloudwatch.Metric {
	if err := p.validateMetricSuccessParameters(props); err != nil {
		panic(err)
	}
	var returns awscloudwatch.Metric

	_jsii_.Invoke(
		p,
		"metricSuccess",
		[]interface{}{props},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) MetricTimeout(props *awscloudwatch.MetricOptions) awscloudwatch.Metric {
	if err := p.validateMetricTimeoutParameters(props); err != nil {
		panic(err)
	}
	var returns awscloudwatch.Metric

	_jsii_.Invoke(
		p,
		"metricTimeout",
		[]interface{}{props},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) NonExecutableCommonArguments(props *SparkJobProps) *map[string]*string {
	if err := p.validateNonExecutableCommonArgumentsParameters(props); err != nil {
		panic(err)
	}
	var returns *map[string]*string

	_jsii_.Invoke(
		p,
		"nonExecutableCommonArguments",
		[]interface{}{props},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) OnEvent(id *string, options *awsevents.OnEventOptions) awsevents.Rule {
	if err := p.validateOnEventParameters(id, options); err != nil {
		panic(err)
	}
	var returns awsevents.Rule

	_jsii_.Invoke(
		p,
		"onEvent",
		[]interface{}{id, options},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) OnFailure(id *string, options *awsevents.OnEventOptions) awsevents.Rule {
	if err := p.validateOnFailureParameters(id, options); err != nil {
		panic(err)
	}
	var returns awsevents.Rule

	_jsii_.Invoke(
		p,
		"onFailure",
		[]interface{}{id, options},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) OnStateChange(id *string, jobState JobState, options *awsevents.OnEventOptions) awsevents.Rule {
	if err := p.validateOnStateChangeParameters(id, jobState, options); err != nil {
		panic(err)
	}
	var returns awsevents.Rule

	_jsii_.Invoke(
		p,
		"onStateChange",
		[]interface{}{id, jobState, options},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) OnSuccess(id *string, options *awsevents.OnEventOptions) awsevents.Rule {
	if err := p.validateOnSuccessParameters(id, options); err != nil {
		panic(err)
	}
	var returns awsevents.Rule

	_jsii_.Invoke(
		p,
		"onSuccess",
		[]interface{}{id, options},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) OnTimeout(id *string, options *awsevents.OnEventOptions) awsevents.Rule {
	if err := p.validateOnTimeoutParameters(id, options); err != nil {
		panic(err)
	}
	var returns awsevents.Rule

	_jsii_.Invoke(
		p,
		"onTimeout",
		[]interface{}{id, options},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) SetupContinuousLogging(role awsiam.IRole, props *ContinuousLoggingProps) interface{} {
	if err := p.validateSetupContinuousLoggingParameters(role, props); err != nil {
		panic(err)
	}
	var returns interface{}

	_jsii_.Invoke(
		p,
		"setupContinuousLogging",
		[]interface{}{role, props},
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) SetupExtraCodeArguments(args *map[string]*string, props *SparkExtraCodeProps) {
	if err := p.validateSetupExtraCodeArgumentsParameters(args, props); err != nil {
		panic(err)
	}
	_jsii_.InvokeVoid(
		p,
		"setupExtraCodeArguments",
		[]interface{}{args, props},
	)
}

func (p *jsiiProxy_PySparkStreamingJob) ToString() *string {
	var returns *string

	_jsii_.Invoke(
		p,
		"toString",
		nil, // no parameters
		&returns,
	)

	return returns
}

func (p *jsiiProxy_PySparkStreamingJob) With(mixins ...constructs.IMixin) constructs.IConstruct {
	args := []interface{}{}
	for _, a := range mixins {
		args = append(args, a)
	}

	var returns constructs.IConstruct

	_jsii_.Invoke(
		p,
		"with",
		args,
		&returns,
	)

	return returns
}

